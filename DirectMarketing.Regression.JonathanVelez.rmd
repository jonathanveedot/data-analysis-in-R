---
title: "Linear Regression: Direct Marketing Analysis"
author: "Jonathan Velez"
date: "January 17, 2017"
output: pdf_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width = 4)
knitr::opts_chunk$set(fig.height = 3)
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(tidy = FALSE)

```

# Introduction

The "DirectMarketing" dataset includes data from a direct marketer who sells his products only via direct mail. He sends catalogs with product characteristics to customers who then order directly from the catalogs. The marketer has developed customer records to learn what makes some customers spend more than others.  

The data set includes n = 1000 customers and the following variables:  

* Age: customer age -- old, middle, or young
* Gender: male or female
* OwnHome: whether customer owns their home -- own or rent
* Married: single or married
* Location: in terms of distance to nearest store selling similar products -- far or close
* Salary: yearly salary of customer in dollars
* Children: number of children -- 0-3
* History: history of previous purchase volume -- low, medium, high, or NA; NA means customer has not yet completed a purchase
* Catalogs: number of catalogs sent -- 6, 12, 18, or 24
* AmountSpent: the amount spent by the customer in dollars  

The objective is to explain the amount spent by each customer in terms of the provided customer characteristics. Hence, for the resulting model, AmountSpent is the response variable, and Age, Gender, OwnHome, Married, Location, Salary, Children, History, and Catalogs are predictors.  
\newpage

# Data Exploration

## Preparation

```{r, message=FALSE}

# Load required packages
library("knitr")        ## summary table
library("ggplot2")      ## data visualization
library("e1071")        ## skewness
library("car")          ## scatter plot matrix
library("leaps")        ## regression subset selection
library("lars")         ## least absolute shrinkage and selection operator

# Load data
data.file = "http://www.yurulin.com/class/spring2017_datamining/data/DirectMarketing.csv"
df = read.csv(data.file, header = TRUE, sep = ',')

# Identify any missing values and handle missing data appropriately
summary(df)

## History contains missing values, but it is known that this means the customer 
## has yet to make a purchase. Since these are not actually missing observations, 
## add a new level named "NewCustomer" into History.
levels(df$History) = c(levels(df$History), "NewCustomer")
df$History[is.na(df$History)] = "NewCustomer"
summary(df$History)

```
\newpage

## Quantitative Data

A table describing the central tendency and spread of each quantitative variable is included below.  

The density distributions of features measured in monetary amounts are known to be sources of skewed distributions. Plotting the density of AmountSpent reveals a unimodal distribution with positive non-zero skewness. The skewness is greater than +1, indicating that the distribution is highly skewed in the positive direction. Plotting the density of Salary reveals a bimodal distribution with positive non-zero skewness. In this case, the skewness is between 0 and +0.5, indicating that the distribution is moderately skewed in the positive direction. The Shapiro-Wilk normality test confirms that these distributions are non-normal at a significance level of 0.01. The normal probability plot can additionally be used to explore the normality of these distributions, and these plots indicate the positive skewness for both features. It may be appropriate to apply a log transformation to these features if regression analysis results in a non-normal distribution of residuals.   

Exploring the correlations (three bottom-left values of the correlation matrix) and scatter plots (three bottom-left figures of the scatter plot matrix) between numeric predictors and the response variable reveals some interesting trends. These results indicate that AmountSpent and Salary have a strong positive correlation, AmountSpent and Catalogs have a moderate positive correlation, and AmountSpent and Children have a slight negative correlation.  

```{r}

# Generate a summary table for quantitative features
Salary = c(summary(df$Salary), sd(df$Salary))
Children = c(summary(df$Children), sd(df$Children))
Catalogs = c(summary(df$Catalogs), sd(df$Catalogs))
AmountSpent = c(summary(df$AmountSpent), sd(df$AmountSpent))
tbl = rbind(Salary, Children, Catalogs, AmountSpent)
tbl = as.data.frame(tbl)
colnames(tbl)[7] = c("sd")
kable(tbl, caption = "Table 1: Summary of attributes")
rm(list=c("Salary", "Children", "Catalogs", "AmountSpent", "tbl"))

```
\newpage
```{r}

# Explore the density distribution of AmountSpent
no.y = theme(axis.title.y=element_blank(), ## remove clutter on y axis
             axis.text.y=element_blank(),
             axis.ticks.y=element_blank())
ggplot(df, aes(x=AmountSpent)) + geom_density() + no.y
skewness(df$AmountSpent)
shapiro.test(df$AmountSpent)
qqnorm(df$AmountSpent)
qqline(df$AmountSpent)

```
\newpage
```{r}

# Explore the density distribution of Salary
ggplot(df, aes(x=Salary)) + geom_density() + no.y
skewness(df$Salary)
shapiro.test(df$Salary)
qqnorm(df$Salary)
qqline(df$Salary)

```
\newpage
```{r, fig.width=7.5, fig.height=6}

# Correlations
df.numeric = df[,sapply(df, is.numeric)]
cor(df.numeric)
suppressWarnings(
  scatterplotMatrix(df.numeric, spread=F, lty.smooth=2, main="Scatter Plot Matrix")
)
rm(df.numeric)

```
\newpage

## Qualitative Data

A conditional density plot of the response variable for each categorical predictor is generated. The mean and median of the response for each category of each predictor are additionally observed, and the various categories of each predictor are tested for significant differences in their means using ANOVA and pairwise t-testing.   

The ANOVA table for AmountSpent by Age demonstrates an F-statistic of 116.7 with a p-value less than 2e-16, and clearly indicates a rejection of the null hypothesis of equal means for all three age groups. The pairwise t-test indicates significant differences in AmountSpent between Young and Middle groups, and Young and Old groups, but there are no significant differences between Middle and Old groups.  

The ANOVA table for AmountSpent by Gender demonstrates an F-statistic of 42.32 with a p-value equal to 1.22e-10, and clearly indicates a rejection of the null hypothesis of equal means between Male and Female.  

The ANOVA table for AmountSpent by OwnHome demonstrates an F-statistic of 140.1 with a p-value less than 2e-16, and clearly indicates a rejection of the null hypothesis of equal means between Own and Rent.   

The ANOVA table for AmountSpent by Married demonstrates an F-statistic of 292.2 with a p-value less than 2e-16, and clearly indicates a rejection of the null hypothesis of equal means between Married and Single.  

The ANOVA table for AmountSpent by Location demonstrates an F-statistic of 68.03 with a p-value equal to 5.05e-16, and clearly indicates a rejection of the null hypothesis of equal means between Close and Far.   

The ANOVA table for AmountSpent by History demonstrates an F-statistic of 283.2 with a p-value less than 2e-16, and clearly indicates a rejection of the null hypothesis of equal for the four History groups. The pairwise t-test indicates that all mean comparisons are significantly different.   
\newpage

```{r}

# AmountSpent by Age
ggplot(df, aes(x=AmountSpent, fill=Age)) +
  geom_density(alpha=0.5) + no.y

aggregate(AmountSpent~Age, data=df, mean)
aggregate(AmountSpent~Age, data=df, median)

summary(aov(AmountSpent~Age, data=df))

pairwise.t.test(df$AmountSpent, df$Age)

```
\newpage
```{r}

# AmountSpent by Gender
ggplot(df, aes(x=AmountSpent, fill=Gender)) +
  geom_density(alpha=0.5) + no.y

aggregate(AmountSpent~Gender, data=df, mean)
aggregate(AmountSpent~Gender, data=df, median)

summary(aov(AmountSpent~Gender, data=df))

```
\newpage
```{r}

#AmountSpent by OwnHome
ggplot(df, aes(x=AmountSpent, fill=OwnHome)) +
  geom_density(alpha=0.5) + no.y

aggregate(AmountSpent~OwnHome, data=df, mean)
aggregate(AmountSpent~OwnHome, data=df, median)

summary(aov(AmountSpent~OwnHome, data=df))

```
\newpage
```{r}

# AmountSpent by Married
ggplot(df, aes(x=AmountSpent, fill=Married)) +
  geom_density(alpha=0.5) + no.y

aggregate(AmountSpent~Married, data=df, mean)
aggregate(AmountSpent~Married, data=df, median)

summary(aov(AmountSpent~Married, data=df))

```
\newpage
```{r}

# AmountSpent by Location
ggplot(df, aes(x=AmountSpent, fill=Location)) +
  geom_density(alpha=0.5) + no.y

aggregate(AmountSpent~Location, data=df, mean)
aggregate(AmountSpent~Location, data=df, median)

summary(aov(AmountSpent~Location, data=df))

```
\newpage
```{r}

# AmountSpent by History
ggplot(df, aes(x=AmountSpent, fill=History)) +
  geom_density(alpha=0.5) + no.y

aggregate(AmountSpent~History, data=df, mean)
aggregate(AmountSpent~History, data=df, median)

summary(aov(AmountSpent~History, data=df))

pairwise.t.test(df$AmountSpent, df$History)

```
\newpage

# Regression Analysis

## Linear Regression

The linear regression model of AmountSpent against all predictors is statistically significant and accounts for 74.76% of the variance in AmountSpent. The introduction of a penalty for the number of estimated coefficients results in this model explaining 74.46% of the variance in AmountSpent. The leave-one-out cross-validation demonstrates a root mean square error of 489.30, and indicates prediction errors in the magnitude of hundreds of dollars. Coefficients are determined significantly different from zero at the p < 0.001 level. Hence, the coefficients for AgeOld, AgeYoung, GenderMale, OwnHomeRent, MarriedSingle, and HistoryNewCustomer are not significant. The coefficients for LocationFar, Salary, Children, HistoryLow, HistoryMedium, and Catalogs were found to be significant.   

Evaluating linear regression models with various subsets of predictors reveals that while the linear model including all 9 predictors explains 74.76% of the variance in AmountSpent, models including only four or five predictors achieve comparable performance. A linear regression model of AmountSpent against Location, Salary, Children, and Catalogs results in an R-squared of 0.7148 and root mean square error of 516.25. A linear regression model additionally including History as a predictor results in an R-squared of 0.7462 and root mean square error of 488.34. The detailed results are included below.  

```{r}

# Function to compute RMSE via cross-validation (leave-one-out)
cross.val.rmse = function(data, response, formula) {
  n = length(data[,response])
  diff = NULL
  for(k in 1:n) {
    train = c(1:n)
    train = train[train != k]
    model = lm(formula, data=data[train,])
    predicted = predict(model, newdat=data[-train,])
    observed = data[-train, response]
    diff[k] = observed - predicted
  }
  return(sqrt(mean(diff^2))) ## return RMSE
}

# Linear regression model of AmountSpent against all predictors
f = AmountSpent~.
summary(lm(f, df))
cross.val.rmse(df, "AmountSpent", f)

# Explore linear regression models including various subsets of predictors
models = summary(regsubsets(AmountSpent~., data=df, nbest=1, nvmax=ncol(df)-1))
tbl = cbind(models$which, models$rsq, models$adjr2)[,-1]
tbl = as.data.frame(tbl)
colnames(tbl)[13:14] = c("R2", "Adj.R2")
tbl
rm(list=c("models", "tbl"))

# Linear regression model of AmountSpent against Location, Salary, Children, Catalogs
f = AmountSpent~Location+Salary+Children+Catalogs
summary(lm(f, df))
cross.val.rmse(df, "AmountSpent", f)

# Linear regression model of AmountSpent against Location, Salary, Children, Catalogs, 
# and History
f = AmountSpent~Location+Salary+Children+Catalogs+History
summary(lm(f, df))
cross.val.rmse(df, "AmountSpent", f)

```
\newpage

## Polynomial Regression

The scatter plot of AmountSpent against Salary appears to somewhat follow a quadratic trend line that spreads as Salary increases. However, plotting the training sample prediction error and cross-validation prediction error of polynomial regression over various degrees suggests that polynomial regression would not offer any significant gains. An instance of polynomial regression that models Salary as a 2nd degree polynomial, Catalogs as a 3rd degree polynomial, and additionally includes Children, Location, and History as linear terms results in a model that accounts for 74.68% of the variance and produces a root mean square error of 490.40. This performance is comparable but not superior to the linear model. Furthermore, the summary of this polynomial regression model indicates that the coefficients of the quadratic and cubic terms are likely to not be significantly different from zero. These results indicate that the linear regression model is superior.  

```{r, message=FALSE, fig.width=6}

ggplot(df, aes(y=AmountSpent, x=Salary))+
    geom_point()

# Function for comparing in-sample and out-of-sample error of 
# polynomial regression over various degrees
cross.val.poly.reg = 
  function(data, response, poly.var, lin.var, deg=12, train.set=0.5) {
    ## measure performance in terms of RMSE
    rmse = function(y, p) { return(sqrt(mean((y - p)^2))) }
    performance = data.frame()
    ## split data into a training set and test set for cross-validation
    n = length(data[,response])
    train = sort(sample(1:n, round(train.set*n)))
    formula = as.formula(paste(response,"~poly(",poly.var,", degree=d)+",lin.var,sep=""))
    
    for (d in 1:deg) {
      poly.fit = lm(formula, data=data[train,])
      performance = rbind(performance, 
                          data.frame(Degree=d, Error="Training", 
                                     RMSE = rmse(data[train,response],
                                                 predict(poly.fit))
                                     )
                          )
      performance = rbind(performance, 
                          data.frame(Degree=d, Error="Cross-Validation",
                                     RMSE = rmse(data[-train,response], 
                                                 predict(poly.fit, newdata=data[-train,]))
                                     )
                          )
      }
  
    ## Plot the performance of polynomial regression models for each degree
    require("ggplot2")
    require("scales")
    ggplot(performance , aes(x=Degree, y=RMSE, linetype=Error)) + 
      geom_point() + geom_line() + scale_y_continuous(labels=comma)
}

set.seed(13)
cross.val.poly.reg(df, "AmountSpent", "Salary+Catalogs", "Children+Location+History", deg=6)

# An instance of polynomial regression (summary and out-of-sample RMSE)
f = AmountSpent~poly(Salary, degree=2)+poly(Catalogs, degree=3)+Children+Location+History
summary(lm(f, df))
cross.val.rmse(df, "AmountSpent", f)

```
\newpage

## LASSO

Although regularization is not necessary for modeling this data, it is interesting to examine the variable selection process and additionally confirm the previously selected predictors. The graph of the LASSO estimates as a function of the shrinkage illustrates the order in which variables enter the model as one relaxes the constraint on the L1 norm of their estimates. The first variable to enter is Salary, then Catalogs, followed by HistoryLow, Location, and Children, with the rest of the variables far off. Cross-validation (10-fold) indicates that the error is minimized at 0.8 of the final L1 norm. The results of LASSO confirm the selected set of predictors, but regularization would not offer any significant performance gains.  

```{r}

x = model.matrix(AmountSpent~., data=df)
x = x[,-1] ## remove the intercept
lasso = lars(x = x, y = df$AmountSpent, trace = TRUE)
lasso
```
\newpage
```{r}
plot(lasso)
coef(lasso, s=c(.20, .40, .60, .80, 1.0), mode="fraction")
```
\newpage
```{r, fig.height=5.5, fig.width=5}
cv.lars(x=x, y=df$AmountSpent, K=10)
rm(list=c("x", "lasso"))

```
\newpage

## Validation of Linear Regression

The normal probability plot of the standardized residuals indicates the non-normality of their distribution, and violates the assumption of normality. There is no a priori reason to believe that the amount spent by one customer is influenced by the amount spent by another customer, so the assumption of independence is met. The scatter plot of residuals against fitted values presents somewhat of a curved line that transitions into random noise, indicating that the model may not meet the assumption of linearity and a term may need to be added to the model. The scatter plot of scale against location presents a random band around a curved line, indicating the violation of the assumption of homoscedasticity.  

The exploration of the density distributions of quantitative features previously revealed that AmountSpent is highly skewed and Salary is moderately skewed (both in the positive direction). Applying a log transformation to these monetary features is justified given that the assumptions of linear regression do not hold. A log base-10 transformation of these features offers more organization for visual inspection of the graphed data. This transformation results in all assumptions of linear regression being satisfied. The linear regression model using transformed monetary data accounts for 87.33% of the variation, but the model estimates become more difficult to interpret.  

```{r, fig.height=4.75}

lin.model = lm(AmountSpent~Salary+Catalogs+Children+Location+History, df)
par(mfrow=c(2,2)) ## format plots in 2 by 2 figure
plot(lin.model)
par(mfrow=c(1,1)) ## reset format

```
\newpage
```{r, fig.width=6}

ggplot(df, aes(x=log10(Salary)+Catalogs+Children, y=log10(AmountSpent), 
               shape=Location, color=History)) +
  geom_point(alpha = 0.4, size=2) +
  scale_shape_manual(values=c(16,17))

```
\newpage
```{r}

lin.model = lm(log10(AmountSpent)~log10(Salary)+Catalogs+Children+Location+History, df)
summary(lin.model)

```
\newpage
```{r, fig.height=4.75}

par(mfrow=c(2,2)) ## format plots in 2 by 2 figure
plot(lin.model)
par(mfrow=c(1,1)) ## reset format

```
\newpage

# Results

The regression analysis indicates that customer spending behavior can be predicted by their salary, the number of catalogs they have received, the number of children they have, whether they live close or far to the nearest competitor, and their history of previous purchase volume. Customers that live far from the closest competitor or have a history of high previous purchase volume tend to spend more. Salary is the strongest predictor of spending behavior, followed by the number of catalogs the customer has received and location. The strength of a predictor is determined with respect to the increase in error resulting from its exclusion in the model.  

The linear regression model describing these findings accounts for 74.62% of the variation in customer spending behavior with a root mean square error of $488.34. A model that represents the amount spent and salary of each customer using base-10 logarithms has improved performance and accounts for 87.33% of the variation, but the model estimates become more difficult to interpret. These findings suggest that it may be beneficial to target customers that live close to competitors with more catalogs. Customers that live close to competitors with a low or medium previous purchase volume history tend to demonstrate low spending behavior regardless of salary. It may be advantageous to direct marketing efforts toward these customers in an attempt to boost sales.  

```{r, fig.width=6}

# Graph of AmountSpent against Salary, Catalogs, Children, Location, and History
ggplot(df, aes(x=Salary+Catalogs+Children, y=AmountSpent, 
               shape=Location, color=History)) +
  geom_point(alpha = 0.4, size=2) +
  scale_shape_manual(values=c(16,17))

```
\newpage
```{r, fig.width=5}

# Linear Regression Trend Line
ggplot(df, aes(x=Salary+Catalogs+Children, y=AmountSpent)) +
  geom_point(alpha = 0.4, size=2) +
  geom_smooth(method="lm")

```
\newpage
```{r, fig.width=6, fig.height=4.5}

# Linear Regression Trend Lines by Location for AmountSpent against 
# Salary, Catalogs, and Children (faceted on History)
ggplot(df, aes(x=Salary+Catalogs+Children, y=AmountSpent, color=Location)) +
  geom_point(alpha = 0.4, size=2) +
  geom_smooth(method="lm") +
  facet_wrap(~History)

```
\newpage
```{r}
# Observation of the increase in RMSE as each variable is excluded
# from the model to determine the most important predictor

## baseline
cross.val.rmse(df, "AmountSpent", AmountSpent~Salary+Catalogs+Children+Location+History)

## exclude Salary
cross.val.rmse(df, "AmountSpent", AmountSpent~Catalogs+Children+Location+History)

## exclude Catalogs
cross.val.rmse(df, "AmountSpent", AmountSpent~Salary+Children+Location+History)

## exclude Children
cross.val.rmse(df, "AmountSpent", AmountSpent~Salary+Catalogs+Location+History)

## exclude Location
cross.val.rmse(df, "AmountSpent", AmountSpent~Salary+Catalogs+Children+History)

## exclude History
cross.val.rmse(df, "AmountSpent", AmountSpent~Salary+Catalogs+Children+Location)

```
\newpage